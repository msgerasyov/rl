# -*- coding: utf-8 -*-
"""DRQN_new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cOSOilpuiHBCmnWu-rGW_f3X5gonVuUN
"""

import random
import numpy as np
import torch

# Commented out IPython magic to ensure Python compatibility.
import gym
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

ENV_NAME = "BreakoutNoFrameskip-v4"

from gym.core import ObservationWrapper
from gym.spaces import Box

import cv2

class PreprocessAtariObs(ObservationWrapper):
    def __init__(self, env, color=False):
        """A gym wrapper that crops, scales image into the desired shapes and optionally grayscales it."""
        ObservationWrapper.__init__(self, env)

        self.color = color
        self.img_size = (1, 64, 64)
        self.observation_space = Box(0.0, 1.0, (1, 64, 64))

    def _to_gray_scale(self, rgb_image, channel_weights=[0.8, 0.1, 0.1]):
        img_gray = np.zeros(rgb_image.shape[:-1], dtype='float32')
        for i in range(len(channel_weights)):
          img_gray += channel_weights[i] * rgb_image[:,:,i]
        return img_gray[np.newaxis,:,:]

    def observation(self, img):
        """what happens to each observation"""
        img = img[25:200]
        img = cv2.resize(img, self.img_size[1:])
        if not self.color:
            img = img.mean(-1, keepdims=True)

        return img.transpose([2, 0, 1]) / 255


class MaxAndSkipEnv(gym.Wrapper):
    def __init__(self, env, skip=4):
        """Return only every `skip`-th frame"""
        gym.Wrapper.__init__(self, env)
        # most recent raw observations (for max pooling across time steps)
        self._obs_buffer = np.zeros(
            (2,) + env.observation_space.shape, dtype=np.uint8)
        self._skip = skip

    def step(self, action):
        """Repeat action, sum reward, and max over last observations."""
        total_reward = 0.0
        done = None
        for i in range(self._skip):
            obs, reward, done, info = self.env.step(action)
            if i == self._skip - 2:
                self._obs_buffer[0] = obs
            if i == self._skip - 1:
                self._obs_buffer[1] = obs
            total_reward += reward
            if done:
                break
        # Note that the observation on the done=True frame
        # doesn't matter
        max_frame = self._obs_buffer.max(axis=0)

        return max_frame, total_reward, done, info

    def reset(self, **kwargs):
        return self.env.reset(**kwargs)


class ClipRewardEnv(gym.RewardWrapper):
    def __init__(self, env):
        gym.RewardWrapper.__init__(self, env)

    def reward(self, reward):
        """Bin reward to {+1, 0, -1} by its sign."""
        return np.sign(reward)


class FireResetEnv(gym.Wrapper):
    def __init__(self, env):
        """Take action on reset for environments that are fixed until firing."""
        gym.Wrapper.__init__(self, env)
        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'
        assert len(env.unwrapped.get_action_meanings()) >= 3

    def reset(self, **kwargs):
        self.env.reset(**kwargs)
        obs, _, done, _ = self.env.step(1)
        if done:
            self.env.reset(**kwargs)
        obs, _, done, _ = self.env.step(2)
        if done:
            self.env.reset(**kwargs)
        return obs

    def step(self, ac):
        return self.env.step(ac)


class EpisodicLifeEnv(gym.Wrapper):
    def __init__(self, env):
        """Make end-of-life == end-of-episode, but only reset on true game over.
        Done by DeepMind for the DQN and co. since it helps value estimation.
        """
        gym.Wrapper.__init__(self, env)
        self.lives = 0
        self.was_real_done = True

    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        self.was_real_done = done
        # check current lives, make loss of life terminal,
        # then update lives to handle bonus lives
        lives = self.env.unwrapped.ale.lives()
        if lives < self.lives and lives > 0:
            # for Qbert sometimes we stay in lives == 0 condition for a few frames
            # so it's important to keep lives > 0, so that we only reset once
            # the environment advertises done.
            done = True
        self.lives = lives
        return obs, reward, done, info

    def reset(self, **kwargs):
        """Reset only when lives are exhausted.
        This way all states are still reachable even though lives are episodic,
        and the learner need not know about any of this behind-the-scenes.
        """
        if self.was_real_done:
            obs = self.env.reset(**kwargs)
        else:
            # no-op step to advance from terminal/lost life state
            obs, _, _, _ = self.env.step(0)
        self.lives = self.env.unwrapped.ale.lives()
        return obs


# in torch imgs have shape [c, h, w] instead of common [h, w, c]
class AntiTorchWrapper(gym.ObservationWrapper):
    def __init__(self, env):
        gym.ObservationWrapper.__init__(self, env)

        self.img_size = [env.observation_space.shape[i]
                         for i in [1, 2, 0]
                         ]
        self.observation_space = gym.spaces.Box(0.0, 1.0, self.img_size)

    def observation(self, img):
        """what happens to each observation"""
        img = img.transpose(1, 2, 0)
        return img


class FrameBuffer(gym.Wrapper):
    def __init__(self, env, n_frames=4, dim_order='tensorflow'):
        """A gym wrapper that reshapes, crops and scales image into the desired shapes"""
        super(FrameBuffer, self).__init__(env)
        self.dim_order = dim_order
        if dim_order == 'tensorflow':
            height, width, n_channels = env.observation_space.shape
            obs_shape = [height, width, n_channels * n_frames]
        elif dim_order == 'pytorch':
            n_channels, height, width = env.observation_space.shape
            obs_shape = [n_channels * n_frames, height, width]
        else:
            raise ValueError(
                'dim_order should be "tensorflow" or "pytorch", got {}'.format(dim_order))
        self.observation_space = Box(0.0, 1.0, obs_shape)
        self.framebuffer = np.zeros(obs_shape, 'float32')

    def reset(self):
        """resets breakout, returns initial frames"""
        self.framebuffer = np.zeros_like(self.framebuffer)
        self.update_buffer(self.env.reset())
        return self.framebuffer

    def step(self, action):
        """plays breakout for 1 step, returns frame buffer"""
        new_img, reward, done, info = self.env.step(action)
        self.update_buffer(new_img)
        return self.framebuffer, reward, done, info

    def update_buffer(self, img):
        if self.dim_order == 'tensorflow':
            offset = self.env.observation_space.shape[-1]
            axis = -1
            cropped_framebuffer = self.framebuffer[:, :, :-offset]
        elif self.dim_order == 'pytorch':
            offset = self.env.observation_space.shape[0]
            axis = 0
            cropped_framebuffer = self.framebuffer[:-offset]
        self.framebuffer = np.concatenate(
            [img, cropped_framebuffer], axis=axis)

def PrimaryAtariWrap(env, clip_rewards=True):
    assert 'NoFrameskip' in env.spec.id

    # This wrapper holds the same action for <skip> frames and outputs
    # the maximal pixel value of 2 last frames (to handle blinking
    # in some envs)
    env = MaxAndSkipEnv(env, skip=4)

    # This wrapper sends done=True when each life is lost
    # (not all the 5 lives that are givern by the game rules).
    # It should make easier for the agent to understand that losing is bad.
    env = EpisodicLifeEnv(env)

    # This wrapper laucnhes the ball when an episode starts.
    # Without it the agent has to learn this action, too.
    # Actually it can but learning would take longer.
    env = FireResetEnv(env)

    # This wrapper transforms rewards to {-1, 0, 1} according to their sign
    if clip_rewards:
        env = ClipRewardEnv(env)

    # This wrapper is yours :)
    env = PreprocessAtariObs(env)
    return env

def make_env(clip_rewards=True, seed=None):
    env = gym.make(ENV_NAME)  # create raw env
    if seed is not None:
        env.seed(seed)
    env = PrimaryAtariWrap(env, clip_rewards)
    return env


import torch
import torch.nn as nn
import torch.nn.functional as F
#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# those who have a GPU but feel unfair to use it can uncomment:
device = torch.device('cpu')
device

class DQNAgent(nn.Module):
    def __init__(self, state_shape, n_actions, device, lstm_size = 128, epsilon=0):

        super().__init__()
        self.epsilon = epsilon
        self.n_actions = n_actions
        self.state_shape = state_shape
        self.lstm_size = lstm_size
        self.device = device


        # Define your network body here. Please make sure agent is fully contained here
        # nn.Flatten() can be useful

        self.conv = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, stride=2),
            nn.ReLU(),
            nn.Conv2d(32, 32, kernel_size=3, stride=2),
            nn.ReLU(),
            nn.Conv2d(32, 32, kernel_size=3, stride=2),
            nn.ReLU(),
            nn.Conv2d(32, 32, kernel_size=3, stride=2),
            nn.ReLU(),
        )

        self.lstm = nn.LSTMCell(self.feature_size(), self.lstm_size)

        self.val = nn.Linear(self.lstm_size, 1)
        self.adv = nn.Linear(self.lstm_size, n_actions)

        self.to(device)

    def feature_size(self):
        return self.conv(torch.zeros(1, *self.state_shape)).view(1, -1).size(1)

    def get_initial_state(self, batch_size):
        return torch.zeros((batch_size, self.lstm_size), device=self.device), \
                torch.zeros((batch_size, self.lstm_size), device=self.device)

    def forward(self, prev_state, obs_t):
        """
        takes agent's observation (tensor), returns qvalues (tensor)
        :param state_t: a batch of 4-frame buffers, shape = [batch_size, 4, h, w]
        """
        # Use your network to compute qvalues for given state
        #print(state_t.shape)
        h = self.conv(obs_t)

        h = h.view(h.size(0), -1)

        new_state = h_new, c_new = self.lstm(h, prev_state)
        advantage = self.adv(h_new)
        value = self.val(h_new)


        adv_mean = torch.mean(advantage, dim=1, keepdim=True)
        qvalues = value + advantage - adv_mean

        return new_state, qvalues

    def get_qvalues(self, prev_state, obs_t):
        """
        like forward, but works on numpy arrays, not tensors
        """
        obs_t = torch.tensor(obs_t, device=self.device, dtype=torch.float)
        (h, c), qvalues = self.forward(prev_state, obs_t)
        return (h.detach(), c.detach()), qvalues.data.cpu().numpy()

    def sample_actions(self, qvalues):
        """pick actions given qvalues. Uses epsilon-greedy exploration strategy. """
        epsilon = self.epsilon
        batch_size, n_actions = qvalues.shape

        random_actions = np.random.choice(n_actions, size=batch_size)
        best_actions = qvalues.argmax(axis=-1)

        should_explore = np.random.choice(
            [0, 1], batch_size, p=[1-epsilon, epsilon])
        return np.where(should_explore, random_actions, best_actions)

def evaluate(env, agent, n_games=1, greedy=False):
    """Plays an entire game start to end, returns session rewards."""
    game_rewards = []
    for _ in range(n_games):
  # initial observation and memory
        observation = env.reset()
        prev_memories = agent.get_initial_state(1)

        total_reward = 0
        while True:
            new_memories, qvalues = agent.get_qvalues(prev_memories, observation[None, ...])
            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]

            observation, reward, done, info = env.step(action)

            total_reward += reward
            prev_memories = new_memories
            if done:
              break

        game_rewards.append(total_reward)

    return np.mean(game_rewards)

from collections import deque

class ReplayBuffer(object):
    def __init__(self, max_epi_num=10e4, max_epi_len=10):
        # capacity is the maximum number of episodes
        self.max_epi_num = max_epi_num
        self.max_epi_len = max_epi_len
        self.memory = deque(maxlen=self.max_epi_num)
        self.is_av = False
        self.current_epi = 0
        self.memory.append([])

    def reset(self):
        self.current_epi = 0
        self.memory.clear()
        self.memory.append([])

    def create_new_epi(self):
        if len(self.memory) >= self.max_epi_num:
            self.memory.popleft()
            self.current_epi -= 1
        self.memory.append([])
        self.current_epi += 1

    def add(self, state, action, reward, done):
        if len(self.memory[self.current_epi]) >= self.max_epi_len:
            self.create_new_epi()
        self.memory[self.current_epi].append([state, action, reward, done])


    def sample(self):
        epi_index = random.randint(0, len(self.memory) - 2)
        epi = self.memory[epi_index]
        states, actions, rewards, dones = [], [], [], []
        for i in range(len(epi)):
            data = epi[i]
            state, action, reward, done = data
            states.append(np.array(state, copy=False))
            actions.append(np.array(action, copy=False))
            rewards.append(reward)
            dones.append(done)
        return (
            np.array(states),
            np.array(actions),
            np.array(rewards),
            np.array(dones)
        )

    def __len__(self):
        return len(self.memory)

    def print_info(self):
        for i in range(len(self.memory)):
            print('epi', i, 'length', len(self.memory[i]))

def play_and_record(initial_obs, agent, env, exp_replay, prev_memories, n_steps=1):
    """
    Play the game for exactly n steps, record every (s,a,r,s', done) to replay buffer.
    Whenever game ends, add record with done=True and reset the game.
    It is guaranteed that env has done=False when passed to this function.

    PLEASE DO NOT RESET ENV UNLESS IT IS "DONE"

    :returns: return sum of rewards over time and the state in which the env stays
    """
    s = initial_obs
    sum_rewards = 0

    # Play the game for n_steps as per instructions above
    for _ in range(n_steps):
        new_memories, qvalues = agent.get_qvalues(prev_memories, s[None, ...])
        a = agent.sample_actions(qvalues)[0]
        next_s, r, done, _ = env.step(a)
        exp_replay.add(s, a, r, done)

        s = next_s
        prev_memories = new_memories
        sum_rewards += r
        if done:
            s = env.reset()
            prev_memories = agent.get_initial_state(1)

    return sum_rewards, s, prev_memories

def compute_td_loss(states, actions, rewards, is_done,
                    agent, target_network,
                    gamma=0.99,
                    device=device):
    """ Compute td loss using torch operations only. Use the formulae above. """
    states = torch.tensor(states, device=device, dtype=torch.float)    # shape: [batch_size, *state_shape]

    # for some torch reason should not make actions a tensor
    actions = torch.tensor(actions, device=device, dtype=torch.long)    # shape: [batch_size]
    rewards = torch.tensor(rewards, device=device, dtype=torch.float)  # shape: [batch_size]
    # shape: [batch_size, *state_shape]
    is_done = torch.tensor(
        is_done.astype('float32'),
        device=device,
        dtype=torch.float
    )  # shape: [batch_size]
    is_not_done = 1 - is_done
    min_history_size = (len(actions) - 1) // 2

    agent_memories = agent.get_initial_state(1)
    target_memories = target_network.get_initial_state(1)

    agent_qvalues = []
    target_qvalues = []

    for t in range(len(actions)):

        agent_memories, predicted_agent_qvalues = agent(agent_memories,
                                                            states[t].unsqueeze(0))
        target_memories, predicted_target_qvalues = target_network(target_memories,
                                                            states[t].unsqueeze(0))

        if t >= min_history_size:
            agent_qvalues.append(predicted_agent_qvalues)
            target_qvalues.append(predicted_target_qvalues)

        if is_done[t]:
            agent_memories = agent.get_initial_state(1)
            target_memories = target_network.get_initial_state(1)

    agent_qvalues = torch.stack(agent_qvalues, dim=1).squeeze()
    target_qvalues = torch.stack(target_qvalues, dim=1).squeeze()[1:,:]

    agent_next_qvalues = agent_qvalues[1:,:]
    best_actions = torch.argmax(agent_next_qvalues, dim=1)

    predicted_qvalues_for_actions = agent_qvalues[range(
            len(actions[min_history_size:])), actions[min_history_size:]][:-1]
    next_state_values = target_qvalues[range(
            len(best_actions)), best_actions]

    target_qvalues_for_actions = rewards[min_history_size:-1] \
                + gamma * is_not_done[min_history_size:-1] * next_state_values


    loss = torch.mean((predicted_qvalues_for_actions -
                    target_qvalues_for_actions.detach()) ** 2)

    return loss

from tqdm import trange

import numpy as np
import psutil
from scipy.signal import convolve, gaussian
import torch
from torch import nn
import os

def linear_decay(init_val, final_val, cur_step, total_steps):
    if cur_step >= total_steps:
        return final_val
    return (init_val * (total_steps - cur_step) +
            final_val * cur_step) / total_steps


def smoothen(values):
    kernel = gaussian(100, std=100)
    # kernel = np.concatenate([np.arange(100), np.arange(99, -1, -1)])
    kernel = kernel / np.sum(kernel)
    return convolve(values, kernel, 'valid')

if __name__ == "__main__":
    #from tqdm.notebook import trange, tqdm

    seed = 42
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

    env = make_env(seed)
    state_shape = env.observation_space.shape
    n_actions = env.action_space.n
    state = env.reset()

    agent = DQNAgent(state_shape, n_actions, device, epsilon=1.0)
    #agent.load_state_dict(torch.load('/content/gdrive/My Drive/DQN.weights'))
    target_network = DQNAgent(state_shape, n_actions, device)
    target_network.load_state_dict(agent.state_dict())

    exp_replay = ReplayBuffer(10**2, 10)
    play_and_record(state, agent, env, exp_replay, agent.get_initial_state(1), n_steps=10**3)

    print(len(exp_replay))

    timesteps_per_epoch = 1
    total_steps = 3 * 10**6
    decay_steps = 10**6

    opt = torch.optim.Adam(agent.parameters(), lr=1e-4)

    init_epsilon = 1.0
    final_epsilon = 0.1

    loss_freq = 50
    refresh_target_network_freq = 5000
    eval_freq = 5000

    max_grad_norm = 50

    mean_rw_history = []
    td_loss_history = []
    grad_norm_history = []
    initial_state_v_history = []
    step = 0

    state = env.reset()
    prev_memories = agent.get_initial_state(1)
    for step in trange(step, total_steps + 1):

        agent.epsilon = linear_decay(init_epsilon, final_epsilon, step, decay_steps)

        # play
        _, state, prev_memories = play_and_record(state, agent, env, exp_replay,
                                                  prev_memories, timesteps_per_epoch)

        # train
        obs_batch, act_batch, reward_batch, is_done_batch = exp_replay.sample()

        loss = compute_td_loss(obs_batch, act_batch, reward_batch,
                               is_done_batch, agent, target_network)

        loss.backward()
        grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)
        opt.step()
        opt.zero_grad()

        if step % loss_freq == 0:
            td_loss_history.append(loss.data.cpu().item())
            grad_norm_history.append(grad_norm)

        if step % refresh_target_network_freq == 0:
            # Load agent weights into target_network
            target_network.load_state_dict(agent.state_dict())

        if step % eval_freq == 0:
            torch.save(agent.state_dict(), 'DRQN.weights')
            mean_rw_history.append(np.mean(evaluate(
                make_env(clip_rewards=True, seed=step), agent, n_games=5, greedy=True))
            )
            _, initial_state_q_values = agent.get_qvalues(agent.get_initial_state(1),
                [make_env(seed=step).reset()]
            )
            initial_state_v_history.append(np.max(initial_state_q_values))

            print(step, mean_rw_history[-1], initial_state_v_history[-1],
                    td_loss_history[-1], grad_norm_history[-1])
